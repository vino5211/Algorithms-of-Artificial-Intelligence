# View of GANs

### gan zoo
+ https://github.com/hindupuravinash/the-gan-zoo.git

### 独家 | GAN大盘点，聊聊这些年的生成对抗网络 : LSGAN, WGAN, CGAN, infoGAN, EBGAN, BEGAN, VAE
- http://nooverfit.com/wp/%E7%8B%AC%E5%AE%B6%EF%BD%9Cgan%E5%A4%A7%E7%9B%98%E7%82%B9%EF%BC%8C%E8%81%8A%E8%81%8A%E8%BF%99%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-lsgan-wgan-cgan-info/


- 资源 | textgenrnn：只需几行代码即可训练文本生成网络
  - 本文是一个 GitHub 项目，介绍了 textgenrnn，一个基于 Keras/TensorFlow 的 Python 3 模块。只需几行代码即可训练文本生成网络
  - 项目地址：https://github.com/minimaxir/textgenrnn?reddit=1
  - textgenrnn 是一个基于 Keras/TensorFlow 的 Python 3 模块，用于创建 char-rnn，具有许多很酷炫的特性：
  	- 它是一个使用注意力权重（attention-weighting）和跳跃嵌入（skip-embedding）等先进技术的现代神经网络架构，用于加速训练并提升模型质量。
  	- 能够在字符层级和词层级上进行训练和预测。
  	- 能够设置 RNN 的大小、层数，以及是否使用双向 RNN。
  	- 能够对任何通用的输入文本文件进行训练。
  	- 能够在 GPU 上训练模型，然后在 CPU 上使用这些模型。
  	- 在 GPU 上训练时能够使用强大的 CuDNN 实现 RNN，这比标准的 LSTM 实现大大加速了训练时间。
  	- 能够使用语境标签训练模型，能够更快地学习并在某些情况下产生更好的结果。
  + Tweet Generator：训练一个为任意数量的 Twitter 用户生成推文而优化的神经网络

### 对抗样本和对抗网络 
+ http://www.lancezhange.com/2015/11/19/adversarial-samples/
+ 17种GAN变体的Keras实现
	+ https://blog.csdn.net/JohinieLi/article/details/79595146 
+ 干货｜生成式对抗网络GAN的研究进展与展望
  + http://www.sohu.com/a/167048525_642762

+ Ian Goodfellow推荐的10篇GAN进展跟踪文献
	+ https://weibo.com/ttarticle/p/show?id=2309404212119326295632
	+ 1. Progressive GANs: Progressive Growing of GANs for Improved Quality, Stability, and Variation
	+ 2. Spectral normalization: Spectral Normalization for Generative Adversarial Networks (got GANs working on lots of classes, which has been hard)
	+ 3. Projection discriminator: cGANs with Projection Discriminator (from the same lab as #2, both techniques work well together, overall give very good results with 1000 classes)
	+ 4. pix2pixHD High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (GANs for 2-megapixel video)​
	+ 5. Are GANs created equal? Are GANs Created Equal? A Large-Scale Study  A big empirical study showing the importance of good rigorous empirical work and how a lot of the GAN variants don't seem to actually offer improvements in practice​
	+ 6. WGAN-GP Improved Training of Wasserstein GANs  : probably the most popular GAN variant today and seems to be pretty good in my opinion. Caveat: the baseline GAN variants should not perform nearly as badly as this paper claims, especially the text one​
	+ 7. StackGAN++: StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks  High quality text-to-image synthesis with GANs​
	+ 8. Making all ML algorithms differentially private by training them on fake private data generated by GANs​ Privacy-preserving generative deep neural networks support clinical data sharing
	+ 9. You should be a little bit aware of the "GANs with encoders" space, one of my favorites is Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks​
	+ 10. You should be a little bit aware of the "theory of GAN convergence" space, one of my favorites is Gradient descent GAN optimization is locally stable​​​​
