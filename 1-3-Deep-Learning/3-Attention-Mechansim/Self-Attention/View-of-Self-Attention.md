
- Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction
  - Self-Attention
  - 本文是 Andrew McCallum 团队应用 Self-Attention 在生物医学关系抽取任务上的一个工作。这篇论文作者提出了一个文档级别的生物关系抽取模型，作者使用 Google 提出包含 Self-Attention 的 transformer 来对输入文本进行表示学习，和原始的 transformer 略有不同在于他们使用了窗口大小为 5 的 CNN 代替了原始 FNN。
  - 代码链接
  	- https://github.com/patverga/bran
- DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding
  - 本文是悉尼科技大学发表于 AAAI 2018 的工作，这篇文章是对 Self-Attention 的另一种应用，作者提出一种新的方向性的 Attention，从而能更加有效地理解语义。
  - 代码链接
  	- https://github.com/shaohua0116/Group-Normalization-Tensorflow
