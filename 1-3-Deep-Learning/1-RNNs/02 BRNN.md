# BRNN
### Reference
+ https://blog.csdn.net/App_12062011/article/details/76837680

### Outline
+ 如果能像访问过去的上下文信息一样，访问未来的上下文，这样对于许多序列标注任务是非常有益的。例如，在最特殊字符分类的时候，如果能像知道这个字母之前的字母一样，知道将要来的字母，这将非常有帮助。同样，对于句子中的音素分类也是如此
+ 然而，由于标准的循环神经网络（RNN）在时序上处理序列，他们往往忽略了未来的上下文信息。一种很显而易见的解决办法是在输入和目标之间添加延迟，进而可以给网络一些时步来加入未来的上下文信息，也就是加入M时间帧的未来信息来一起预测输出。理论上，M可以非常大来捕获所有未来的可用信息，但事实上发现如果M过大，预测结果将会变差。这是因为网路把精力都集中记忆大量的输入信息，而导致将不同输入向量的预测知识联合的建模能力下降。因此，M的大小需要手动来调节
+ 双向循环神经网络（BRNN）的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络（RNN），而且这两个都连接着一个输出层。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一个时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层（w1, w3），隐含层到隐含层自己（w2, w5），向前和向后隐含层到输出层（w4, w6）。值得注意的是：向前和向后隐含层之间没有信息流，这保证了展开图是非循环的

![](https://img-blog.csdn.net/20160721152411328)