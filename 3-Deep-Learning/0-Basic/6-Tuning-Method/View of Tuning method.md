# View of Tuning Method
### Reference
+ https://blog.csdn.net/scutjy2015/article/details/72810730
+ https://zhidao.baidu.com/question/2143667585344442628.html
	+ 学习率fixed lr从0.01到10的-6或-7就可以了
	+ 参数初始化：高斯  若某两层的梯度计算相差太大，就要调整小的那层的std了
	+ 激活函数relu+bn
	+ 数据预处理方式：zero-center
	+ 梯度裁剪： 限制最大梯度 或者设置阀值，让梯度强制等于10,20等
	+ 对于小的数据集，dropout=0.5和L2正则化项，效果不错
	+ 优化方法：SGD+Momentum 效果往往可以胜过adam等，虽然adam收敛更快，据说初始几轮momentum设置的小一点会更好，这点待验证。
	+ rnn trick batch size=1效果会更好（待验证）</ol>9.early stop 或者 有个初始模型接着训练
	+ 参数初始化：可以先用随机初始化的方式训练一个简单的网络模型，再将训练好的权值初始化给复杂的网络模型，复杂的网络模型采用高斯初始化可能会更好（VGG） 

+ https://blog.csdn.net/kokozeng1995/article/details/78640055

```
基本原则:
快速试错
一些大的注意事项:
刚开始, 先上小规模数据, 模型往大了放, 只要不爆显存, 能用256个filter你就别用128个. 直接奔着过拟合去. 没错, 就是训练过拟合网络, 连测试集验证集这些都可以不用.
为什么?

你要验证自己的训练脚本的流程对不对. 这一步小数据量, 生成速度快, 但是所有的脚本都是和未来大规模训练一致的(除了少跑点循环)
如果小数据量下, 你这么粗暴的大网络奔着过拟合去都没效果. 那么, 你要开始反思自己了, 模型的输入输出是不是有问题? 要不要检查自己的代码(永远不要怀疑工具库, 除非你动过代码)? 模型解决的问题定义是不是有问题? 你对应用场景的理解是不是有错? 不要怀疑NN的能力, 不要怀疑NN的能力, 不要怀疑NN的能力. 就我们调参狗能遇到的问题, NN没法拟合的, 这概率是有多小?
你可以不这么做, 但是等你数据准备了两天, 结果发现有问题要重新生成的时候, 你这周时间就酱油了.
2.Loss设计要合理.

一般来说分类就是Softmax, 回归就是L2的loss. 但是要注意loss的错误范围(主要是回归), 你预测一个label是10000的值, 模型输出0, 你算算这loss多大, 这还是单变量的情况下. 一般结果都是nan. 所以不仅仅输入要做normalization, 输出也要这么弄.
多任务情况下, 各loss想法限制在一个量级上, 或者最终限制在一个量级上, 初期可以着重一个任务的loss
3.观察loss胜于观察准确率 准确率虽然是评测指标, 但是训练过程中还是要注意loss的. 你会发现有些情况下, 准确率是突变的, 原来一直是0, 可能保持上千迭代, 然后突然变1. 要是因为这个你提前中断训练了, 只有老天替你惋惜了. 而loss是不会有这么诡异的情况发生的, 毕竟优化目标是loss.

给NN一点时间, 要根据任务留给NN的学习一定空间. 不能说前面一段时间没起色就不管了. 有些情况下就是前面一段时间看不出起色, 然后开始稳定学习.

4.确认分类网络学习充分 分类网络就是学习类别之间的界限. 你会发现, 网络就是慢慢的从类别模糊到类别清晰的. 怎么发现? 看Softmax输出的概率的分布. 如果是二分类, 你会发现, 刚开始的网络预测都是在0.5上下, 很模糊. 随着学习过程, 网络预测会慢慢的移动到0,1这种极值附近. 所以, 如果你的网络预测分布靠中间, 再学习学习.

5.Learning Rate设置合理

太大: loss爆炸, 或者nan 
+ 太小: 半天loss没反映(但是, LR需要降低的情况也是这样, 这里可视化网络中间结果, 不是weights, 有效果, 俩者可视化结果是不一样的, 太小的话中间结果有点水波纹或者噪点的样子, 因为filter学习太慢的原因, 试过就会知道很明显)
需要进一步降低了: loss在当前LR下一路降了下来, 但是半天不再降了.
如果有个复杂点的任务, 刚开始, 是需要人肉盯着调LR的. 后面熟悉这个任务网络学习的特性后, 可以扔一边跑去了.
如果上面的Loss设计那块你没法合理, 初始情况下容易爆, 先上一个小LR保证不爆, 等loss降下来了, 再慢慢升LR, 之后当然还会慢慢再降LR, 虽然这很蛋疼.
LR在可以工作的最大值下往小收一收, 免得ReLU把神经元弄死了. 当然, 我是个心急的人, 总爱设个大点的.
6.对比训练集和验证集的loss 
判断过拟合, 训练是否足够, 是否需要early stop的依据, 这都是中规中矩的原则, 不多说了.

7.清楚receptive field的大小 
CV的任务, context window是很重要的. 所以你对自己模型的receptive field的大小要心中有数. 这个对效果的影响还是很显著的. 特别是用FCN, 大目标需要很大的receptive field. 不像有fully connection的网络, 好歹有个fc兜底, 全局信息都有.

简短的注意事项:
1.预处理: -mean/std zero-center就够了, PCA, 白化什么的都用不上. 我个人观点, 反正CNN能学习encoder, PCA用不用其实关系不大, 大不了网络里面自己学习出来一个. 
2.shuffle, shuffle, shuffle. 
3.网络原理的理解最重要, CNN的conv这块, 你得明白sobel算子的边界检测. 
4.Dropout, Dropout, Dropout(不仅仅可以防止过拟合, 其实这相当于做人力成本最低的Ensemble, 当然, 训练起来会比没有Dropout的要慢一点, 同时网络参数你最好相应加一点, 对, 这会再慢一点). 
5.CNN更加适合训练回答是否的问题, 如果任务比较复杂, 考虑先用分类任务训练一个模型再finetune. 
6.无脑用ReLU(CV领域). 
7.无脑用3x3. 
8.无脑用xavier. 
9.LRN一类的, 其实可以不用. 不行可以再拿来试试看. 
10.filter数量2^n. 
11.多尺度的图片输入(或者网络内部利用多尺度下的结果)有很好的提升效果. 
12.第一层的filter, 数量不要太少. 否则根本学不出来(底层特征很重要). 
13.sgd adam 这些选择上, 看你个人选择. 一般对网络不是决定性的. 反正我无脑用sgd + momentum. 
14.batch normalization我一直没用, 虽然我知道这个很好, 我不用仅仅是因为我懒. 所以要鼓励使用batch normalization. 
15.不要完全相信论文里面的东西. 结构什么的觉得可能有效果, 可以拿去试试. 
16.你有95%概率不会使用超过40层的模型. 
17.shortcut的联接是有作用的. 
18.暴力调参最可取, 毕竟, 自己的生命最重要. 你调完这个模型说不定过两天这模型就扔掉了. 
19.机器, 机器, 机器. 
20.Google的inception论文, 结构要好好看看. 
21.一些传统的方法, 要稍微了解了解. 我自己的程序就用过1x14的手写filter, 写过之后你看看inception里面的1x7, 7x1 就会会心一笑…
```

+ https://blog.csdn.net/chenzhi1992/article/details/52905569

```
经常会被问到你用深度学习训练模型时怎么样改善你的结果呢？然后每次都懵逼了，一是自己懂的不多，二是实验的不多，三是记性不行忘记了。所以写这篇博客，记录下别人以及自己的一些经验。

Ilya Sutskever（Hinton的学生）讲述了有关深度学习的见解及实用建议：

获取数据：确保要有高质量的输入/输出数据集，这个数据集要足够大、具有代表性以及拥有相对清楚的标签。缺乏数据集是很难成功的。

预处理：将数据进行集中是非常重要的，也就是要使数据均值为0，从而使每个维度的每次变动为1。有时，当输入的维度随量级排序变化时，最好使用那个维度的log(1+x)。基本上，重要的是要找到一个0值的可信编码以及自然分界的维度。这样做可使学习工作得更好。情况就是这样的，因为权值是通过公式来更新的：wij中的变化 \propto xidL/dyj（w表示从层x到层y的权值，L是损失函数）。如果x的均值很大（例如100），那么权值的更新将会非常大，并且是相互关联的，这使得学习变得低劣而缓慢。保持0均值和较小的方差是成功的关键因素。

批处理：在如今的计算机上每次只执行一个训练样本是很低效的。反之如果进行的是128个例子的批处理，效率将大幅提高，因为其输出量是非常可观的。事实上使用数量级为1的批处理效果不错，这不仅可获得性能的提升同时可降低过度拟合；不过这有可能会被大型批处理超越。但不要使用过大的批处理，因为有可能导致低效和过多过度拟合。所以我的建议是：根据硬件配置选取适合的批处理规模，量力而为会更加高效。

梯度归一化：根据批处理的大小来拆分梯度。这是一个好主意，因为如果对批处理进行倍增（或倍减），无需改变学习率（无论如何，不要太多）。

学习率计划：从一个正常大小的学习率（LR）开始，朝着终点不断缩小。

1LR的典型取值是0.1，令人惊讶的是，对于大量的神经网络问题来说，0.1是学习率的一个很好的值。通常学习率倾向于更小而非更大。
使用一个验证集——一个不进行训练的训练集子集，来决定何时降低学习率以及何时停止训练（例如当验证集的错误开始增多的时候）。
学习率计划的实践建议：若发现验证集遭遇瓶颈，不妨将LR除以2（或5），然后继续。最终，LR将会变得非常小，这也到了停止训练的时候了。这样做可以确保在验证性能受到损害的时候，你不会拟合（或过度拟合）训练数据。降低LR是很重要的，通过验证集来控制LR是个正确的做法。
但最重要的是要关注学习率。一些研究人员（比如Alex Krizhevsky）使用的方法是，监视更新范数和权值范数之间的比率。比率取值大约为10¯³。如果取值过小，那么学习会变得非常慢；如果取值过大，那么学习将会非常不稳定甚至失败。

权值初始化。关注权值在学习开始时的随机初始化。

如果想偷懒，不妨试试0.02*randn(num_params)。这个范围的值在许多不同的问题上工作得很好。当然，更小（或更大）的值也值得一试。
如果它工作得不好（例如是一个非常规的和/或非常深的神经网络架构），那么需要使用init_scale/sqrt(layer_width)*randn来初始化每个权值矩阵。在这种情况下，init_scale应该设置为0.1或者1，或者类似的值。
对于深度且循环的网络，随机初始化是极其重要的。如果没有处理好，那么它看起来就像没有学习到任何东西。我们知道，一旦条件都设置好了，神经网络就会学习。
一个有趣的故事：多年来，研究人员相信SGD不能训练来自随机初始化的深度神经网络。每次尝试都以失败告终。令人尴尬的是，他们没有成功是因为使用“小的随机权值”来进行初始化，虽然小数值的做法在浅度网络上工作得非常好，但在深度网络上的表现一点也不好。当网络很深时，许多权值矩阵之间会进行乘积，所以不好的结果会被放大。
但如果是浅度网络，SGD可以帮助我们解决该问题。
所以关注初始化是很有必要的。尝试多种不同的初始化，努力就会得到回报。如果网络完全不工作（即没法实施），继续改进随机初始化是正确的选择。

如果正在训练RNN或者LSTM，要对梯度（记得梯度已除以批量大小）范数使用一个硬约束。像15或者5这样的约束在我个人的实验中工作得很好。请将梯度除以批处理大小，再检查一下它的范数是否超过15（或5）。如果超过了，将它缩小到15（或5）。这个小窍门在RNN和LSTM的训练中发挥着巨大作用，不这样做的话，爆炸性的梯度将会导致学习失败，最后不得不使用像1e-6这样微小而无用的学习率。

数值梯度检查：如果没有使用过Theano或者Torch，梯度实现只能亲力亲为了。在实现梯度的时候很容易出错，所以使用数值梯度检查是至关重要的。这样做会让你对自己的代码充满信心。调整超级参数（比如学习率和初始化）是非常有价值的，因此好刀要用在刀刃上。

如果正在使用LSTM同时想在具有大范围依赖的问题上训练它们，那么应该将LSTM遗忘关口的偏差初始化为较大的值。默认状态下，遗忘关口是S型的全部输入，当权值很小时，遗忘关口会被设置为0.5，这只能对部分问题有效。这是对LSTM初始化的一个警示。

数据增加（Data augmentation）：使用算法来增加训练实例数量是个有创意的做法。如果是图像，那么应该转换和旋转它们；如果是音频，应该将清晰的部分和所有类型的杂音进行混合处理。数据添加是一门艺术（除非是在处理图像），这需要一定的常识。

dropout：dropout提供了一个简单的方法来提升性能。记得要调整退出率，而在测试时不要忘记关闭dropout，然后对权值求乘积（也就是1-dropout率）。当然，要确保将网络训练得更久一点。不同于普通训练，在进入深入训练之后，验证错误通常会有所增加。dropout网络会随着时间推移而工作得越来越好，所以耐心是关键。

综合（Ensembling）。训练10个神经网络，然后对其预测数据进行平均。该做法虽然简单，但能获得更直接、更可观的性能提升。有人可能会困惑，为什么平均会这么有效？不妨用一个例子来说明：假如两个分类器的错误率为70%，如果其中一个的正确率保持较高，那么平均后的预测会更接近正确结果。这对于可信网络的效果会更加明显，当网络可信时结果是对的，不可信时结果是错的。

（下面几点是上面的简化版）
1：准备数据：务必保证有大量、高质量并且带有干净标签的数据，没有如此的数据，学习是不可能的
2：预处理：这个不多说，就是0均值和1方差化
3：minibatch：建议值128,1最好，但是效率不高，但是千万不要用过大的数值，否则很容易过拟合
4：梯度归一化：其实就是计算出来梯度之后，要除以minibatch的数量。这个不多解释
5：下面主要集中说下学习率
5.1：总的来说是用一个一般的学习率开始，然后逐渐的减小它
5.2：一个建议值是0.1，适用于很多NN的问题，一般倾向于小一点。
5.3：一个对于调度学习率的建议：如果在验证集上性能不再增加就让学习率除以2或者5，然后继续，学习率会一直变得很小，到最后就可以停止训练了。
5.4：很多人用的一个设计学习率的原则就是监测一个比率（每次更新梯度的norm除以当前weight的norm），如果这个比率在10-3附近，如果小于这个值，学习会很慢，如果大于这个值，那么学习很不稳定，由此会带来失败。
6：使用验证集，可以知道什么时候开始降低学习率，和什么时候停止训练。
7：关于对weight初始化的选择的一些建议：
7.1：如果你很懒，直接用0.02*randn(num_params)来初始化，当然别的值你也可以去尝试
7.2：如果上面那个不太好使，那么久依次初始化每一个weight矩阵用init_scale / sqrt(layer_width) * randn,init_scale可以被设置为0.1或者1
7.3：初始化参数对结果的影响至关重要，要引起重视。
7.4：在深度网络中，随机初始化权重，使用SGD的话一般处理的都不好，这是因为初始化的权重太小了。这种情况下对于浅层网络有效，但是当足够深的时候就不行了，因为weight更新的时候，是靠很多weight相乘的，越乘越小，有点类似梯度消失的意思（这句话是我加的）
8：如果训练RNN或者LSTM，务必保证gradient的norm被约束在15或者5（前提还是要先归一化gradient），这一点在RNN和LSTM中很重要。
9：检查下梯度，如果是你自己计算的梯度。
10：如果使用LSTM来解决长时依赖的问题，记得初始化bias的时候要大一点
12：尽可能想办法多的扩增训练数据，如果使用的是图像数据，不妨对图像做一点扭转啊之类的，来扩充数据训练集合。
13：使用dropout
14：评价最终结果的时候，多做几次，然后平均一下他们的结果。
```