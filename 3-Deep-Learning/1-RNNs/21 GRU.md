# Gate Recurrent Unit

### Reference
+ https://zhuanlan.zhihu.com/p/32481747

### Outline
+ 为了解决长期记忆和反向传播中的梯度等问题而提出来的
+ 与LSTM 相比：
	+ 我们在我们的实验中选择GRU是因为它的实验效果与LSTM相似，但是更易于计算
	+ R-NET: MACHINE READING COMPREHENSION WITH SELF-MATCHING NETWORKS（2017）

+ 输入输出结构
	+　GRU的输入输出结构与普通的RNN是一样的。
	+ 有一个当前的输入 $x^t$ ，和上一个节点传递下来的隐状态（hidden state） $h^{t-1}$ ，这个隐状态包含了之前节点的相关信息。
	+ 结合 $x^t$  和 $h^{t-1}$，GRU会得到当前隐藏节点的输出 $y^t$  和传递给下一个节点的隐状态 $h^t$
	![](https://pic2.zhimg.com/80/v2-49244046a83e30ef2383b94644bf0f31_hd.jpg)
+ ### 内部结构
	+ 首先，我们先通过上一个传输下来的状态 h^{t-1} 和当前节点的输入 x^t 来获取两个门控状态
	+ 如下图所示，其中 r  控制重置的门控（reset gate）， z 为控制更新的门控（update gate）
	
        ![](https://pic4.zhimg.com/80/v2-7fff5d817530dada1b279c7279d73b8a_hd.jpg)
        
        + 得到门控信号之后，首先使用重置门控来得到“重置”之后的数据 ${h^{t-1}}' = h^{t-1} \odot r$  ，再将 ${h^{t-1}}'$ 与输入 $x^t$  进行拼接，再通过一个tanh激活函数来将数据放缩到-1~1的范围内。即得到如下图2-3所示的 h'

	![](https://pic1.zhimg.com/80/v2-390781506bbebbef799f1a12acd7865b_hd.jpg)
        
        ![](https://pic2.zhimg.com/80/v2-8134a00c243153bfd9fd2bcbe0844e9c_hd.jpg)
        
        + 上图中的 $\odot$ 是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 $\oplus$ 则代表进行矩阵加法操作
        + ### 更新记忆
        + 更新表达式： $h^t = z \odot h^{t-1} + (1 - z)\odot h'$
        + 强调一下，门控信号（这里的 z ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多

        + GRU很聪明的一点就在于，我们使用了同一个门控 z 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）
        + $z \odot h^{t-1}$ ：表示对原本隐藏状态的选择性“遗忘”。这里的 z 可以想象成遗忘门（forget gate），忘记 $h^t$ 维度中一些不重要的信息
        + $(1-z) \odot h'$ ： 表示对包含当前节点信息的 h' 进行选择性”记忆“。与上面类似，这里的 (1-z) 同理会忘记 h ' 维度中的一些不重要的信息。或者，这里我们更应当看做是对 h'  维度中的某些信息进行选择
        + $h^t = z \odot h^{t-1} + (1 - z)\odot h'$ ：结合上述，这一步的操作就是忘记传递下来的 h^{t-1}  中的某些维度信息，并加入当前节点输入的某些维度信息
        + 可以看到，这里的遗忘 z 和选择 (1-z) 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （z ），我们就会使用包含当前输入的 h' 中所对应的权重进行弥补 (1-z) 。以保持一种”恒定“状态