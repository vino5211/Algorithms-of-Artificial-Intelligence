# ML Algorithm

## Reference
+ https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice
+ http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/
+ https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf
+ https://www.zhihu.com/question/26726794/answer/421409624
+ 机器学习算法集锦：从贝叶斯到深度学习及各自优缺
	+ https://zhuanlan.zhihu.com/p/25327755

## Repersentation, Evaluation, Optimization
+ 对于supervised learning的机器学习算法而言，机器学习算法可以拆解为representation、evaluation和optimization三个部分。
+ 具体的，假设 x 是训练集中一条sample的data， y 为该条sample的label， $ y^{\hat{}}=f(x;w) $为预测值，那么：
	+ <1> f(x;w) 的具体形式就是representation，比如是一次型的 $ f(x;w)=w^Tx+b $，或者二次型的 $ f(x;w)=x^Twx+b $
	+ <2> 衡量 y 和 $ y^{\hat{}} $ 之间差距的是evaluation，其实也就是loss function，例如我们熟知的squared loss， 
	$$ loss(y, y^\hat{}) = \frac{1}2||y^\hat{}-y||^2= \frac{1}2||f(x;w)-y||^2 $$
        
       ![](https://pic2.zhimg.com/80/v2-dbce9350c5050e1154f0b7c54ff7fd31_hd.jpg)
       
       + <3>根据<2>中的evaluation求解权重 w 的过程则是optimization，包括我们熟知的SGD、EM等都可以划到optimization。

## 为一个具体场景选择算法
+ 搞清楚这个场景的数据分布
	+ 找到representation和该分布契合的模型，例如该场景的数据分布是一次型的，那我们就可以选择logistic regression、SVM等分界面为一次型的模型；如果场景的数据分布是二次型的，我们可以选择naive bayes；如果场景的数据分布既不是一次型也不是二次型，那我们可以选择基于决策树的模型，例如gbdt、random forest等，或者DNN，这些模型都高度非线性，表达能力极强，理论上可以拟合任意曲线
+ 该模型的optimization过程硬件能否承受
	+ 如果场景数据分布是一次型，但是训练集数据量极大，那我们一般会选logistic regression，而放弃SVM，因为SVM的optimization过程对大数据量不太友好。
+ 具体来说
	+ 具体地，logistic regression和SVM（linear kernel）的representation都是一次型的，它们不同的地方在于evaluation和optimization，如果数据分布是一次型的，用这两个差别不会太大，但是logistic regression的optimization过程对大数据量更加友好，而且预测值能有概率意义，所以工业界使用logistic regression更多
	+ 另外一个工业界用得非常多的模型是gbdt，它的representation类似于下图。其实基于决策树的模型都是通过一个个平行于坐标轴的平面去拟合训练集的实际分界面，理论上平行于坐标轴的平面能够拟合任意分界面，这一点类似于DNN。实际场景中，数据分界面为非线性的情况占大多数，gbdt一方面继承了决策树的强表达能力，另外一方面又规避决策树variance太大的问题。
	+ adaboost属于ensemble method中boosting方法的一个具体实现，ensemble method包括bagging、boosting和stacking。这些方法在打比赛的时候常用，因为理论上它们一定会带来效果上的提升。


### SVM
+ Number of features
For certain types of data, the number of features can be very large compared to the number of data points. This is often the case with genetics or textual data. The large number of features can bog down some learning algorithms, making training time unfeasibly long. Support Vector Machines are particularly well suited to this case (see below).

### Bayesian methods
Bayesian methods have a highly desirable quality: they avoid overfitting. They do this by making some assumptions beforehand about the likely distribution of the answer. Another byproduct of this approach is that they have very few parameters. Azure Machine Learning has both Bayesian algorithms for both classification (Two-class Bayes' point machine) and regression (Bayesian linear regression). Note that these assume that the data can be split or fit with a straight line.

Naive Bayes: It is a classification technique based on Bayes’ theorem and very easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods. Naive Bayes is also a good choice when CPU and memory resources are a limiting factor

### 输出错误结果, 从中分析方法的好坏
+ 如何？

### https://www.quora.com/What-is-the-best-way-to-choose-algorithms-for-a-classification-problem-in-machine-learning
There are three steps that come to my mind. You might have to use all the three. Let me call them: i) Data Exploration, ii) Rules-of-thumb, ii) Trial-and-error. Let me attempt to explain these methods sequentially:

i) Data Exploration: What does your data tell you? Can you plot them in 2D and observe some emerging patterns (for higher dimensions, you can use t-SNE)? How many classes are there (binary or multi-way)? You can use other plots such as scatter plot etc. to make yourself acquainted with the data. The reason you should do this exercise is that the data is the best guide that you currently possess. Once you get sufficient insights, you should then move on to the next step, where you employ pre-processing or choose the algorithm for classification. Expert practitioners often replace this step by reading relevant papers on how other people handled this kind of data. However, this part of the exercise might still be needed in the middle of frantically searching for the next experiment during your trial-and-error session.

ii) Rules-of-thumb: Data insights (or insights from reading papers) lead way to decide the next steps: i) pre-processing requirements such as data cleaning or augmentation, ii) the class of algorithm (supervised/semi-supervised); and then iii) the classification algorithm to employ. A finite set of robust rules would be nearly impossible to list. But, let me try to point out a few that should offer a flavor: 1) say your data is binary and well-separated in plots, you might simply stick to SVMs (experimenting with kernels); 2) if you have a large class-imbalance problem (class-1 with 1000 samples and class-2 with 10), employ strategies such as data augmentation etc.; 3) if your total number of samples are not large enough, try to use semi-supervised learning strategies such as active learning etc.

So, when do you apply the different classification techniques that you so feverishly understood: i) Linear Regression: they are for regression problems i.e. when the output is continuous, ii) Decision Tree: when you have a well-defined meaningful set of features and when you know that specific ranges of the features correspond to specific classes, iii) SVM: prior to deep learning approaches, SVM (and its variants) dominated the Machine Learning algorithms - they will be the choice if the dataset is not large enough to employ Deep Learning approaches (or there are other hindrances such as lack of hardware).

Deep Learning approaches again come with their own standard rules of thumb. For example, for text classification one can start by looking at RNN/LSTM and their variants; for image classification, one can start with CNN (and variants). Note, people often employ CNN for text classification with impressive accuracy. How did they come up with the idea? My answer would be by analyzing the data i.e. understanding the inherent properties (and discriminating aspects) of natural language text and examine whether CNN layers can help in better representing some of those discriminating aspects. For example, convolution and max-pooling together are believed to help in capturing long range dependencies.

Again, the above are by no means complete. All these rules of thumb come from experience and one can mine them by reading relevant papers (relevant to the dataset or the type of data you have).

iii) Trial and Error: Last, but not the least (and I guess employed by experienced ML practitioners too), experiment! There is unfortunately no shortcuts here and I can hardly mention any general rule other than: read lots of papers related to your dataset. Experimentation however can vary from changing the classification algorithm (from logistic regression ro SVM) or just changing between variants of the same algorithm (SVM gaussian kernel to other kernels). This again will depend on your understanding of the relevant papers. The more you read, the more possibility is that you will make the right choices in all the above steps.
