# XGBoost:
### 改进:
+ 改进残差函数 不用Gini作为残差，用二阶泰勒展开+树的复杂度（正则项） 带来如下好处：
	1. 可以控制树的复杂度
	2. 带有关于梯度的更多信息，获得了二阶导数
	3. 可以用线性分类器
+ 采用预排序 因为每一次迭代中，都要生成一个决策树，而这个决策树是残差的决策树，所以传统的不能并行 但是陈天奇注意到，每次建立决策树，在分裂节点的时候，比如选中A特征，就要对A进行排序，再计算残差，这个花很多时间 于是陈天奇想到，每一次残差计算好之后，全部维度预先排序，并且此排序是可以并行的，并行排序好后，对每一个维度，计算一次最佳分裂点，求出对应的残差增益 于是只要不断选择最好的残差作为分裂点就可以。 也就是说，虽然森林的建立是串行的没有变，但是每一颗树枝的建立就变成是并行的了，带来的好处：
	1. 分裂点的计算可并行了，不需要等到一个特征的算完再下一个了
	2. 每层可以并行： 当分裂点的计算可以并行，对每一层，比如分裂了左儿子和右儿子，那么这两个儿子上分裂哪个特征及其增益也计算好了
+ Shrinkage（缩减） 相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）
+ 列抽样 XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算。